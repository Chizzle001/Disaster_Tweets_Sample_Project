{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import resample\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Load the data\n",
    "train_df = pd.read_csv('/content/train.csv')\n",
    "test_df = pd.read_csv('/content/test.csv')\n",
    "\n",
    "# Step 2: Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize Stemmer and Lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Step 3: Define text cleaning functions\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'#', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetical characters\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Apply Stemming, Lemmatization, Stopword Removal\n",
    "def preprocess_text(text):\n",
    "    text = clean_text(text)\n",
    "    words = word_tokenize(text)\n",
    "    # Apply Lemmatization and Stemming\n",
    "    words = [lemmatizer.lemmatize(stemmer.stem(word)) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
    "test_df['text'] = test_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Step 4: N-grams (Unigrams, Bigrams, Trigrams)\n",
    "def extract_ngrams(text, n=2):\n",
    "    tokens = word_tokenize(text)\n",
    "    n_grams = ngrams(tokens, n)\n",
    "    return [' '.join(gram) for gram in n_grams]\n",
    "\n",
    "# Example: Adding bigrams and trigrams as features\n",
    "train_df['bigrams'] = train_df['text'].apply(lambda x: extract_ngrams(x, n=2))\n",
    "train_df['trigrams'] = train_df['text'].apply(lambda x: extract_ngrams(x, n=3))\n",
    "\n",
    "test_df['bigrams'] = test_df['text'].apply(lambda x: extract_ngrams(x, n=2))\n",
    "test_df['trigrams'] = test_df['text'].apply(lambda x: extract_ngrams(x, n=3))\n",
    "\n",
    "# Step 5: Data splitting\n",
    "X = train_df['text']\n",
    "y = train_df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: TF-IDF with N-grams\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1, 3))  # Using bigrams and trigrams\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Step 7: Handle class imbalance using resampling\n",
    "def balance_classes(df):\n",
    "    df_majority = df[df['target'] == 1]\n",
    "    df_minority = df[df['target'] == 0]\n",
    "    df_minority_upsampled = resample(\n",
    "        df_minority,\n",
    "        replace=True,\n",
    "        n_samples=len(df_majority),\n",
    "        random_state=42\n",
    "    )\n",
    "    return pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "df_upsampled = balance_classes(train_df)\n",
    "X_upsampled = df_upsampled['text']\n",
    "y_upsampled = df_upsampled['target']\n",
    "X_train_upsampled, X_test_upsampled, y_train_upsampled, y_test_upsampled = train_test_split(\n",
    "    X_upsampled, y_upsampled, test_size=0.2, random_state=42)\n",
    "X_train_tfidf_upsampled = tfidf.fit_transform(X_train_upsampled)\n",
    "X_test_tfidf_upsampled = tfidf.transform(X_test_upsampled)\n",
    "\n",
    "# Step 8: Logistic Regression Model\n",
    "model_lr = LogisticRegression(max_iter=200)\n",
    "model_lr.fit(X_train_tfidf_upsampled, y_train_upsampled)\n",
    "y_pred_lr = model_lr.predict(X_test_tfidf_upsampled)\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "print(\"Logistic Regression Model Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_upsampled, y_pred_lr):.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test_upsampled, y_pred_lr)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test_upsampled, y_pred_lr)}\")\n",
    "\n",
    "# Step 9: Support Vector Classifier (SVC)\n",
    "model_svc = SVC(kernel='linear', random_state=42)\n",
    "model_svc.fit(X_train_tfidf_upsampled, y_train_upsampled)\n",
    "y_pred_svc = model_svc.predict(X_test_tfidf_upsampled)\n",
    "\n",
    "# Evaluate SVC\n",
    "print(\"Support Vector Classifier Model Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_upsampled, y_pred_svc):.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test_upsampled, y_pred_svc)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test_upsampled, y_pred_svc)}\")\n",
    "\n",
    "# Step 10: Random Forest Classifier\n",
    "model_rf = RandomForestClassifier(random_state=42)\n",
    "model_rf.fit(X_train_tfidf_upsampled, y_train_upsampled)\n",
    "y_pred_rf = model_rf.predict(X_test_tfidf_upsampled)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "print(\"Random Forest Model Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_upsampled, y_pred_rf):.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test_upsampled, y_pred_rf)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test_upsampled, y_pred_rf)}\")\n",
    "\n",
    "# Step 11: Plot Confusion Matrices\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Disaster', 'Disaster'], yticklabels=['Not Disaster', 'Disaster'])\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "\n",
    "# Plot for each model\n",
    "plot_confusion_matrix(y_test_upsampled, y_pred_lr, \"Logistic Regression\")\n",
    "plot_confusion_matrix(y_test_upsampled, y_pred_svc, \"Support Vector Classifier\")\n",
    "plot_confusion_matrix(y_test_upsampled, y_pred_rf, \"Random Forest\")\n",
    "\n",
    "# Step 12: Predict outcomes on test data using Random Forest Model\n",
    "new_tweets = test_df['text']\n",
    "X_test_tfidf_testing = tfidf.transform(new_tweets)\n",
    "predictions = model_rf.predict(X_test_tfidf_testing)\n",
    "\n",
    "# Display predictions\n",
    "for tweet, pred in zip(new_tweets[:10], predictions[:10]):\n",
    "    print(f\"Tweet: {tweet}\")\n",
    "    print(f\"Prediction: {'Real Disaster' if pred == 1 else 'Not Real Disaster'}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the neccessary Libraries\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Importing the neccessary Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import resample\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Loading the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
